{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ficha de trabalho 4\n",
    "\n",
    "1- Considera o dataset cachexia. Lê os dados usando a função read_csv do pandas (dados e metadados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77, 64)\n",
      "(77, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data_metlab = pd.read_csv(r\"/home/tiago/AulasSegundoAno/Repositorio-Sistemas/datasets/data_cachexia.csv\")\n",
    "meta_metlab = pd.read_csv(r\"/home/tiago/AulasSegundoAno/Repositorio-Sistemas/datasets/meta_cachexia.csv\")\n",
    "print(data_metlab.shape)\n",
    "print(meta_metlab.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 -Aplica a transformação necessária para que os dados seja escalonados para ter média 0 e desvio padrão 1. Verifica que as médias de todas as colunas é aproximadamente zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'PIF_178'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m input_data \u001b[38;5;241m=\u001b[39m data_metlab\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      3\u001b[0m output_data \u001b[38;5;241m=\u001b[39m meta_metlab\u001b[38;5;241m.\u001b[39mvalues[:,\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m input_sc \u001b[38;5;241m=\u001b[39m preprocessing\u001b[38;5;241m.\u001b[39mscale(input_data) \u001b[38;5;66;03m#media 0 e desvio padrao = 1\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMedia global: \u001b[39m\u001b[38;5;124m\"\u001b[39m, input_sc\u001b[38;5;241m.\u001b[39mmean())\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDesvio padrao global: \u001b[39m\u001b[38;5;124m\"\u001b[39m, input_sc\u001b[38;5;241m.\u001b[39mstd())\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py:198\u001b[0m, in \u001b[0;36mscale\u001b[0;34m(X, axis, with_mean, with_std, copy)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscale\u001b[39m(X, \u001b[39m*\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, with_mean\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, with_std\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    122\u001b[0m     \u001b[39m\"\"\"Standardize a dataset along any axis.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[39m    Center to the mean and component wise scale to unit variance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m \n\u001b[1;32m    197\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m  \u001b[39m# noqa\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m     X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m    199\u001b[0m         X,\n\u001b[1;32m    200\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    201\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    202\u001b[0m         ensure_2d\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    203\u001b[0m         estimator\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthe scale function\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    204\u001b[0m         dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[1;32m    205\u001b[0m         force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    206\u001b[0m     )\n\u001b[1;32m    207\u001b[0m     \u001b[39mif\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(X):\n\u001b[1;32m    208\u001b[0m         \u001b[39mif\u001b[39;00m with_mean:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:856\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    854\u001b[0m         array \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mastype(dtype, casting\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munsafe\u001b[39m\u001b[39m\"\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    855\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 856\u001b[0m         array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    857\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[1;32m    858\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    859\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[1;32m    860\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'PIF_178'"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "input_data = data_metlab.values\n",
    "output_data = meta_metlab.values[:,0]\n",
    "\n",
    "input_sc = preprocessing.scale(input_data) #media 0 e desvio padrao = 1\n",
    "\n",
    "print(\"Media global: \", input_sc.mean())\n",
    "print(\"Desvio padrao global: \", input_sc.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - Cria dois conjuntos de dados treino e teste para serem usados na criação e validação de modelos de aprendizagem máquina. Considera 30% das amostras para formar o conjunto de teste. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 - Constroi diversos tipos de modelos de classificação treinando e testando com os conjuntos criados anteriormente. Calcula, para cada um dos modelos no test set, as métricas de erro PECC e F1-score, e calcula a matriz de confusão. Considere como classe positiva a classe \"cachexic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 - Usando apenas o melhor modelo do exercício anterior, compare o seu desempenho considerando dados com transformação logaritmica e posteriormente standardizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
